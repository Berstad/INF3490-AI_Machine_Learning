\input{../header.tex}
\title{\vspace{-2cm}INF3490/INF4490 Exercise solutions - Reinforcement Learning}
\author{Eivind Samuelsen\input{../author_footnote.tex}}
\date{}

% Removing paragraph indents is sometimes useful:
\setlength\parindent{0pt}
% ==============================================================================

% ================================= DOCUMENT ===================================
\begin{document}
    \renewcommand\marginsymbol[1][0pt]{%
  \tabto*{0cm}\makebox[-1cm][c]{$\mathbb{P}$}\tabto*{\TabPrevPos}}

\maketitle
\input{../intro.tex}

\section{Policies}
What policy would make on-policy and off-policy learning equivalent,
specifically if we consider Q-learning and SARSA-learning?
In other words, what policy used by an agent will make the learning based on Q-learning and SARSA-learning the same?\\

\textit{Answer:}

Q-learning is an off-policy algorithm and SARSA-learning is an on-policy algorithm.
Assuming the same starting state for each learning episode, if the policy used by and agent is the "greedy policy", then the ensuing states and actions that are taken in these states would be the same for both Q and SARSA learning.
Thus, the "greedy policy" would make on-policy and off-policy learning do the same thing, making them equivalent.

\section{Reinforcement learning for chess}
Imagine you were to design a reinforcement learning agent for playing chess.
The state that the agent sees on its turn is the layout of the chess board.
We can set the reward structure for the agent to be +1 for winning, -1 for losing, 0 for drawing, and 0 again for every move that does not lead to a win or loss.
Such an agent will essentially learn to win.
It will do so eventually after much exploration and a number of episodes, since it is always trying to maximize its expected return (cumulative rewards in the long run).
What might happen if, in addition, we give a positive reward to the agent for taking its opponent's pieces as well?\\

\textit{Answer:}

A positive reward for taking the opponent's pieces can distract the agent into learning a policy that may favore the agent taking the opponent's pieces at the expense of winning the game.
Thus, the agent might learn a apolicy that only helps it take the opponent's pieces, but not one that is a good strategy to win.
It is true that taking the opponent's pieces helps towards a possible win in the game (i.e. a positive reward for taking pieces may indicate the agent how it could win), but taking pieces is not sufficient for a win.
In general, the reward structure should tell the agent what to do, not how to do it.

\section{Large state/action spaces}
In most real world problems with large state/action spaces, quantizing the state/action space and using tables to store/update values, e.g. the Q table, is not feasible.
Can you suggest a way for reinforcement learning algorithms to generalize to arbitrarily large real valued state/action spaces?
Hint: The tables are approximating a value function, i.e. mapping a state-action pair to a value.
What else could be used for function approximation?\\

\textit{Answer:}

When the state/action space is large, one can use a neural network to represent the value function.
The neural network would thus take the state-action par as its input, and give the value of this par as the output.
As such, value updates happen indirectly via updating the weights of the neural network.

\input{../contact.tex}
\end{document}
% ==============================================================================
