\input{../header.tex}
\title{\vspace{-2cm}INF3490/INF4490 Exercise Solutions - Simple search algorithms}
\author{Ole Herman S. Elgesem\input{../author_footnote.tex}}
\date{}

% Removing paragraph indents is sometimes useful:
\setlength\parindent{0pt}
% ==============================================================================

% ================================= DOCUMENT ===================================
\begin{document}
    \renewcommand\marginsymbol[1][0pt]{%
  \tabto*{0cm}\makebox[-1cm][c]{$\mathbb{P}$}\tabto*{\TabPrevPos}}

\maketitle
\input{../intro.tex}

\section{Simple search algorithms}

Given the function \(f(x) = -x^4 + 2x^3 + 2x^2 - x\):

\subsection{Derivative}
What is its derivative \(f'(x)\) ?\\

\textit{Answer:}
\[f'(x) = -4x^3 + 6x^2 + 4x - 1\]

\subsection{Plotting \marginsymbol}
Plot the function, and its gradient(derivative) from \(x=-2\) to \(x=3\).
Use python, wolfram alpha or another plotting tool of your choice.\\

\textit{Plot:}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.6\textwidth]{eps/w1e1b.eps}
\caption{\(f(x)\) and it's derivative.}
\label{fig:w1e1b}
\end{center}
\end{figure}

\textit{Source code (Python 3):}
\inputminted{Python}{py/w1e1b.py}

\subsection{Gradient Ascent \marginsymbol}
\label{subsec:grada}
Maximize using gradient ascent.
You can try step size 0.1 and start somewhere in the range [-2, 3].
How does the choice of starting point and step size affect the algorithm's performance?
Is there a starting point where the algorithm would not even be able to find a local maximum?\\

\textit{Answer:}

Both starting position and step size affects where the algorithm ends:
\begin{itemize}
    \item Starting Position
    \begin{itemize}
        \item \textit{Left side:} Should converge on left maximum
        \item \textit{Center:} Stops immediately, gradient is zero.
        \item \textit{Right side:} Should converge on right maximum
    \end{itemize}
    \item Step Size
    \begin{itemize}
        \item \textit{Too low:} Converges slowly (poor performance)
        \item \textit{Too high:} Overshoot, bounce over solutions. Doesn't converge, might not terminate.
    \end{itemize}
\end{itemize}

\textit{Plot:}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.6\textwidth]{eps/w1e1c.eps}
\caption{Result of gradient ascent}
\label{fig:w1e1c}
\end{center}
\end{figure}

\textit{Source code (Python 3):}
\inputminted{Python}{py/w1e1c.py}

\subsection{Exhaustive Search \marginsymbol}
\label{subsec:exhaust}
Assume that we are only interested in maxima of \(f(x)\) where
\(-2\leq x \leq 3\), and \(x\) increases in steps of length 0.5.
(\(\Delta x = 0.5\)). Perform an exhaustive search to
maximize \(f(x)\) and plot the result.\\

\textit{Plot:}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.6\textwidth]{eps/w1e1d.eps}
\caption{Result of exhaustive search}
\label{fig:w1e1c}
\end{center}
\end{figure}

\textit{Source code (Python 3):}
\inputminted{Python}{py/w1e1d.py}

\subsection{Greedy Search and Hill Climbing}
In what way would greedy search and hill climbing differ for the maximization problem in Problem \ref{subsec:grada}?
Can you identify a starting position where the two algorithms might give different results?\\

\textit{Answer:}

Greedy search will check all neighbors and always go in most promising direction.
Hill climber will randomly find a decent neighbor and walk there.
If both algorithms started in center ( \(x = 0.5\) ),
greedy search always go right,
while hill climber could go right and left ( 50/50 ) as both are \emph{up}.

\subsection{Possible improvements}
Gradient ascent, greedy search and hill climbing are quite similar,
and are all based almost exclusively on exploitation.
Can you think of any additions to these algorithms in order to do more exploration?\\

\textit{Answer:}

Run the algorithm several times with random starting positions,
this will \emph{explore} the solution space and find several local optima.
Another option is to add more random movement to either algorithm.
This can be done after a solution is found, or at a probability while searching.
Could also do backtrack + random jump after a solution is found.

\subsection{Exhaustive search vs. simulated annealing}
Which algorithm do you think is the most efficient at maximizing \(f(x)\) under the conditions in Problem \ref{subsec:exhaust}: exhaustive search or simulated annealing?
Explain.\\

\textit{Answer:}

Exhaustive search is better for this case.
It takes few iterations to go through the one dimensional solution space.
For problems in higher dimensions or more complex functions with smaller step size exhaustive search becomes impractical and simulated annealing will outperform.

\input{../contact.tex}
\end{document}
% ==============================================================================
