\input{../header.tex}
\title{\vspace{-2cm}INF3490/INF4490 Exercises - Neural Networks}
\author{Eivind Samuelsen\input{../author_footnote.tex}}
\date{}

% Removing paragraph indents is sometimes useful:
\setlength\parindent{0pt}
% ==============================================================================

% ================================= DOCUMENT ===================================
\begin{document}
    \renewcommand\marginsymbol[1][0pt]{%
  \tabto*{0cm}\makebox[-1cm][c]{$\mathbb{P}$}\tabto*{\TabPrevPos}}

\maketitle
\input{../intro.tex}

\section{Single Layer Perceptron}
\subsection{}
In the perceptron below, what will the output be when the input is (0, 0)?
What about inputs (0, 1), (1, 1) and (1, 0)?
What if we change the bias weight to -0.5?

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.3\textwidth]{fig1.png}
\caption{Single Layer Perceptron. b = 1 }
\label{fig:slp}
\end{center}
\end{figure}

\subsection{}
Starting with random weights, how do you proceed in order to train the perceptron above to perform any given binary operation?
Explain.
\subsection{\marginsymbol}
Implement the perceptron, and train it to perform the logical functions NOT (use only one of the inputs), NAND, and NOR.
What happens when you try to train it do the XOR function?

\section{Multi Layer Perceptron (MLP)}
The figure below shows a multilayer perceptron that constructs the XOR function.
How would you rewrite it to construct the binary equivalence function
(i.e. the output is above threshold when both inputs are either 0 or 1)?
Can you construct it so that it will detect equivalence for any combination of integer inputs?

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.6\textwidth]{fig2.png}
\caption{An illustrated example of a multi layer perceptron}
\label{fig:mlp}
\end{center}
\end{figure}

\input{../contact.tex}
\end{document}
% ==============================================================================
